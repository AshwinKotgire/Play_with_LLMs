{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP5o4/1x+s7lDKqT/o9b44E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "61f1dde400d34c2ba59cd9e589f02561": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f60a942a814243f2a3ef3fcc9b97689e",
              "IPY_MODEL_31d38091cedd49e6b3305f73c9560b26",
              "IPY_MODEL_b46cf37d4e7146e6b290379ae3ba1d00",
              "IPY_MODEL_7d40c97c3dbf473f85748ddec4979ed2"
            ],
            "layout": "IPY_MODEL_14ae445d4e1b42b29e852872bb633daa"
          }
        },
        "995ce988dc5549c4bb7b1bc9a1408ca9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47cc6e246e2c48569fce4549d712c0cc",
            "placeholder": "​",
            "style": "IPY_MODEL_9678016e4d7f4887879853b204ca059f",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "3c48539a7781412b88827ee6aaa79678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_5be696b2646243648c310adac43f86b9",
            "placeholder": "​",
            "style": "IPY_MODEL_3cdee879cfe64af6b8c1ce8b37562d87",
            "value": ""
          }
        },
        "9c6fa371ef124d78ac6d4848487c296d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_42bbfdc765a54ee3b694381e9ee40593",
            "style": "IPY_MODEL_6a31a52d4cda481ba947bcf2e093045b",
            "value": true
          }
        },
        "b4602d9baf094f9799302598554e69ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_9682d04871d94e86860e0a969ade7790",
            "style": "IPY_MODEL_ddf5085887a845ac88ac789642f61a32",
            "tooltip": ""
          }
        },
        "e28f36462a0b430ca43621030c7871d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e98134086f54458bbd35124b65fbd6ee",
            "placeholder": "​",
            "style": "IPY_MODEL_3ec209097fdd445fb0288b2680a71624",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "14ae445d4e1b42b29e852872bb633daa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "47cc6e246e2c48569fce4549d712c0cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9678016e4d7f4887879853b204ca059f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5be696b2646243648c310adac43f86b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cdee879cfe64af6b8c1ce8b37562d87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42bbfdc765a54ee3b694381e9ee40593": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a31a52d4cda481ba947bcf2e093045b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9682d04871d94e86860e0a969ade7790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ddf5085887a845ac88ac789642f61a32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "e98134086f54458bbd35124b65fbd6ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ec209097fdd445fb0288b2680a71624": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b566be42804d481381491e135cccba6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cc72896d506472085be784665c76939",
            "placeholder": "​",
            "style": "IPY_MODEL_c9fc377442634666b18bc51023e5b018",
            "value": "Connecting..."
          }
        },
        "7cc72896d506472085be784665c76939": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9fc377442634666b18bc51023e5b018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f60a942a814243f2a3ef3fcc9b97689e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0d3c999a74d4467bf36df162868b238",
            "placeholder": "​",
            "style": "IPY_MODEL_de03f90791df447c93abba4c1e86f21b",
            "value": "Token is valid (permission: read)."
          }
        },
        "31d38091cedd49e6b3305f73c9560b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9dab44b57f0421baf3b6a6e373a6dc1",
            "placeholder": "​",
            "style": "IPY_MODEL_577f38b5554243268d2722be5e472772",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "b46cf37d4e7146e6b290379ae3ba1d00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc4d8babbf8e4fc488943e131115a904",
            "placeholder": "​",
            "style": "IPY_MODEL_4e8171c919a7441fbfa4a86445964cc7",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "7d40c97c3dbf473f85748ddec4979ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a35b7644e83f499cac09b347e68ee9d2",
            "placeholder": "​",
            "style": "IPY_MODEL_cdb9d989ef514570a7fb980c508d0a29",
            "value": "Login successful"
          }
        },
        "f0d3c999a74d4467bf36df162868b238": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de03f90791df447c93abba4c1e86f21b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9dab44b57f0421baf3b6a6e373a6dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "577f38b5554243268d2722be5e472772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc4d8babbf8e4fc488943e131115a904": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e8171c919a7441fbfa4a86445964cc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a35b7644e83f499cac09b347e68ee9d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdb9d989ef514570a7fb980c508d0a29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6049a019b2d44faeb49b41084f496fca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0aa6132861d84194ad3887d0b4c7023d",
              "IPY_MODEL_8d72f252bf7b4280b4346ffcbfeb3e39",
              "IPY_MODEL_7840f2694fa14debba01b07140ed5848"
            ],
            "layout": "IPY_MODEL_6175f85e9f7747ad9059233ce758e1ff"
          }
        },
        "0aa6132861d84194ad3887d0b4c7023d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8aa6c4cd615e44c3a2d0557974f3968f",
            "placeholder": "​",
            "style": "IPY_MODEL_739107af75e84d26a0f6923c5ce4d8e7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8d72f252bf7b4280b4346ffcbfeb3e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d90de5c6c40449d5ab943a67813f404a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7c3768beefa4b36a4abbcf54412cd5c",
            "value": 2
          }
        },
        "7840f2694fa14debba01b07140ed5848": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbb2a02322834100a269f1a3ad3dc025",
            "placeholder": "​",
            "style": "IPY_MODEL_62564e7d3101427582157977f7fdd319",
            "value": " 2/2 [01:14&lt;00:00, 33.51s/it]"
          }
        },
        "6175f85e9f7747ad9059233ce758e1ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa6c4cd615e44c3a2d0557974f3968f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "739107af75e84d26a0f6923c5ce4d8e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d90de5c6c40449d5ab943a67813f404a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7c3768beefa4b36a4abbcf54412cd5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbb2a02322834100a269f1a3ad3dc025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62564e7d3101427582157977f7fdd319": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshwinKotgire/Play_with_LLMs/blob/main/Prompt_Engineering/Rough_script_of_LLM_with_context.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##LLM( Llama v2 ) with pdf context\n",
        "In previous 2 notebooks we have individually loaded Llama 7b and FAISS vectorstore . In this we combine both for incontext Q and A\n",
        "\n",
        "##Installing required libraries\n",
        "1. transformers: Hugging face libraries for loading pretrained transformer checkpoints.\n",
        "2. accelerate : Manages communications between CPU and GPU more efficiently.\n",
        "3.datasets : For loading datasets from huggingface for future fine tuning.\n",
        "4. bitsandbytes : Used for quantization of model to run on limited resources.\n",
        "5. einops :Einstein-Inspired Notation for operations\n",
        "6. wandb :weights and biases for visualizations\n",
        "7. Langchain : High level library assisting in creation and deployment of LLM apps : https://python.langchain.com/docs/get_started\n",
        "8. sentence_transformers : SentenceTransformers 🤗 is a Python framework for state-of-the-art sentence, text and image embeddings. : https://huggingface.co/sentence-transformers\n",
        "9. faiss-cpu / faiss-gpu :Facebook AI Similarity Search (Faiss) is a library for efficient similarity search and clustering of dense vectors. : https://python.langchain.com/docs/integrations/vectorstores/faiss\n",
        "10. pypdf : pypdf into array of documents, where each document contains the page content and metadata with page number.: https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf\n",
        "11. tiktoken : Used for tokenwise splitting of string or steam . Tbn that tokenization takes place according to tokens not words.\n",
        "12. Youtube video: LangChain - Using Hugging Face Models locally (code walkthrough)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m6-rGkvjGPvB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rf0UUtIHpzd6",
        "outputId": "64987d9c-7e37-46a4-fb79-6ebe6e911b08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m966.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.5/188.5 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.6/215.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.262-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
            "  Downloading langsmith-0.0.21-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.14 langchain-0.0.262 langsmith-0.0.21 marshmallow-3.20.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m963.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.3.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125925 sha256=a70034c17b71fb37ddefec6122b8b987cf713f155bfe381c69b35ed3a696a938\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, sentence_transformers\n",
            "Successfully installed sentence_transformers-2.2.2 sentencepiece-0.1.99\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.15.0-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.3/270.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.15.0\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U trl transformers accelerate\n",
        "!pip install -q datasets bitsandbytes einops wandb\n",
        "!pip install langchain\n",
        "!pip install sentence_transformers\n",
        "# !pip install faiss-cpu\n",
        "!pip install faiss-gpu\n",
        "!pip install pypdf\n",
        "!pip install  tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "61f1dde400d34c2ba59cd9e589f02561",
            "995ce988dc5549c4bb7b1bc9a1408ca9",
            "3c48539a7781412b88827ee6aaa79678",
            "9c6fa371ef124d78ac6d4848487c296d",
            "b4602d9baf094f9799302598554e69ad",
            "e28f36462a0b430ca43621030c7871d1",
            "14ae445d4e1b42b29e852872bb633daa",
            "47cc6e246e2c48569fce4549d712c0cc",
            "9678016e4d7f4887879853b204ca059f",
            "5be696b2646243648c310adac43f86b9",
            "3cdee879cfe64af6b8c1ce8b37562d87",
            "42bbfdc765a54ee3b694381e9ee40593",
            "6a31a52d4cda481ba947bcf2e093045b",
            "9682d04871d94e86860e0a969ade7790",
            "ddf5085887a845ac88ac789642f61a32",
            "e98134086f54458bbd35124b65fbd6ee",
            "3ec209097fdd445fb0288b2680a71624",
            "b566be42804d481381491e135cccba6c",
            "7cc72896d506472085be784665c76939",
            "c9fc377442634666b18bc51023e5b018",
            "f60a942a814243f2a3ef3fcc9b97689e",
            "31d38091cedd49e6b3305f73c9560b26",
            "b46cf37d4e7146e6b290379ae3ba1d00",
            "7d40c97c3dbf473f85748ddec4979ed2",
            "f0d3c999a74d4467bf36df162868b238",
            "de03f90791df447c93abba4c1e86f21b",
            "d9dab44b57f0421baf3b6a6e373a6dc1",
            "577f38b5554243268d2722be5e472772",
            "dc4d8babbf8e4fc488943e131115a904",
            "4e8171c919a7441fbfa4a86445964cc7",
            "a35b7644e83f499cac09b347e68ee9d2",
            "cdb9d989ef514570a7fb980c508d0a29"
          ]
        },
        "id": "wYmnLhUZqIdS",
        "outputId": "9aba7b1c-9488-4a7e-bf92-03301d889df3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61f1dde400d34c2ba59cd9e589f02561"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers import pipeline\n",
        "import transformers\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "phMqa19CbXkM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####pdftextloader : https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf"
      ],
      "metadata": {
        "id": "5ayUMv9dLjnR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.document_loaders import PDFMinerPDFasHTMLLoader\n",
        "from langchain.document_loaders import PDFPlumberLoader\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from langchain.document_loaders import OnlinePDFLoader\n",
        "\n",
        "\n",
        "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
        "\n"
      ],
      "metadata": {
        "id": "JLnvZBB6j1AG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model.config.use_cache = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6049a019b2d44faeb49b41084f496fca",
            "0aa6132861d84194ad3887d0b4c7023d",
            "8d72f252bf7b4280b4346ffcbfeb3e39",
            "7840f2694fa14debba01b07140ed5848",
            "6175f85e9f7747ad9059233ce758e1ff",
            "8aa6c4cd615e44c3a2d0557974f3968f",
            "739107af75e84d26a0f6923c5ce4d8e7",
            "d90de5c6c40449d5ab943a67813f404a",
            "b7c3768beefa4b36a4abbcf54412cd5c",
            "fbb2a02322834100a269f1a3ad3dc025",
            "62564e7d3101427582157977f7fdd319"
          ]
        },
        "id": "rZ2GRfnkZLJO",
        "outputId": "8d725e61-a26f-4ea7-e28d-6a15e490becb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6049a019b2d44faeb49b41084f496fca"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "if torch.cuda.is_available():\n",
        "    print('cuda')\n",
        "    device = torch.device(\"cuda\")  # If GPU is available, use it.\n",
        "else:\n",
        "    device = torch.device(\"cpu\")   # If GPU is not available, use the CPU.\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWjNIC1Ob9Ns",
        "outputId": "18a08404-cba8-4b01-f052-69b3a14e4332"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations :\n",
        "1. Sometimes the 404 Error occurs where the link to the urls is not accessible even if url is correct . In such case restarting the runtime helped.\n",
        "2. I observed that formating of pypdf was the best . but the rest of the loaders could also be used.\n",
        "PDF loaders : https://python.langchain.com/docs/modules/data_connection/document_loaders/pdf"
      ],
      "metadata": {
        "id": "ruq6Z6RlNSnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 1 : Going Sequentially\n",
        "#### Loading Model --> Loading Embeddings and Vector Store --> Loading Context --> Splitting ,embedding and saving context --> Passing instruction and finding nearest context --> Creating Pormpt --> Passing prompt to LLM."
      ],
      "metadata": {
        "id": "oVm4-T-BWg3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urls=\"https://arxiv.org/pdf/1706.03762.pdf\"\n",
        "loader=PyPDFLoader(urls)\n",
        "data=loader.load()"
      ],
      "metadata": {
        "id": "VquZOqPWrKjJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(data[0])"
      ],
      "metadata": {
        "id": "H9_wtc87e1PC",
        "outputId": "9aad0124-562f-47a0-9242-dd08744cb834",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain.schema.document.Document"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content=' '\n",
        "for i in data : content+=i.page_content\n",
        "content"
      ],
      "metadata": {
        "id": "I-aQ6gio1pju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter=SentenceTransformersTokenTextSplitter(chunk_overlap=50,tokens_per_chunk=100)\n",
        "\n",
        "ss=splitter.split_documents(data)"
      ],
      "metadata": {
        "id": "wumtQLd0_W13",
        "outputId": "29342b0b-987e-4a38-9a28-4c6e3ddffbda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-037dba36247f>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSentenceTransformersTokenTextSplitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_overlap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtokens_per_chunk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text_splitter = TokenTextSplitter(chunk_size=100, chunk_overlap=50)\n",
        "\n",
        "texts = text_splitter.create_documents([content])\n"
      ],
      "metadata": {
        "id": "Qy0e5Tiutl12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "id": "3AwICHUl7-Hq",
        "outputId": "cbe51383-91c3-4ebf-92c1-5c89b717abd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "211"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = HuggingFaceEmbeddings()"
      ],
      "metadata": {
        "id": "x1GCXCs0V5Pg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = FAISS.from_documents(ss, embeddings)"
      ],
      "metadata": {
        "id": "_TQwcYPeWCF9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question=\"What are the parts of transformers?\"\n",
        "contexts=db.similarity_search(question,k=2)\n",
        "print(contexts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_aYuvjFXjWG",
        "outputId": "f507f44c-0057-41a0-d5da-aee9d3313e60"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='figure 1 : the transformer - model architecture. the transformer follows this overall architecture using stacked self - attention and point - wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of figure 1, respectively. 3. 1 encoder and decoder stacks encoder : the encoder is composed of a stack of n = 6 identical layers. each layer has two sub - layers. the first is a multi - head self -', metadata={'source': '/tmp/tmphftgffzl/tmp.pdf', 'page': 2}), Document(page_content='. in this work we employ h = 8 parallel attention layers, or heads. for each of these we use dk = dv = dmodel / h = 64. due to the reduced dimension of each head, the total computational cost is similar to that of single - head attention with full dimensionality. 3. 2. 3 applications of attention in our model the transformer uses multi - head attention in three different ways : • in \" encoder - decoder attention \" layers,', metadata={'source': '/tmp/tmphftgffzl/tmp.pdf', 'page': 4})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input prompt for Llama is of the form :\n",
        "```\n",
        "prompt=\"\"\"<s> [INST]\n",
        "<<SYS>>\n",
        "System guiding message\n",
        "<</SYS>>\n",
        "[/INST]\"\"\"\n",
        "```\n",
        "Sources:\n",
        "1. https://huggingface.co/blog/llama2\n",
        "2. https://discuss.huggingface.co/t/llama-2-7b-hf-repeats-context-of-question-directly-from-input-prompt-cuts-off-with-newlines/48250/10  \n",
        "If not done so will result in the language model misbehaving . Therefore prompt engineering is important."
      ],
      "metadata": {
        "id": "R0yFmEzNL1Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context=' '\n",
        "for i in contexts:context+=i.page_content\n",
        "context"
      ],
      "metadata": {
        "id": "pomQiqh8GDfM",
        "outputId": "ff2f329e-3e54-4c26-cddb-e7118ae46719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' figure 1 : the transformer - model architecture. the transformer follows this overall architecture using stacked self - attention and point - wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of figure 1, respectively. 3. 1 encoder and decoder stacks encoder : the encoder is composed of a stack of n = 6 identical layers. each layer has two sub - layers. the first is a multi - head self -. in this work we employ h = 8 parallel attention layers, or heads. for each of these we use dk = dv = dmodel / h = 64. due to the reduced dimension of each head, the total computational cost is similar to that of single - head attention with full dimensionality. 3. 2. 3 applications of attention in our model the transformer uses multi - head attention in three different ways : • in \" encoder - decoder attention \" layers,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"<s>[INST] <<SYS>>\n",
        "You are an honest assistant , who gives factually correct answers while refering to the context. If the context does not have an answer you try your best to answer the question on your own.\\n\n",
        "Answer the following question while refering the context.If the answer is not in context ,try to answer on your own . Still if you are not able to answer the question then politely say so instead of ramblling.\\n\n",
        "<</SYS>>\n",
        "Context:{context}\n",
        "Qustion:{question}\n",
        " [/INST]\n",
        "\"\"\"\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNYUZKnAWIDw",
        "outputId": "9b297b34-e2a9-4489-8f9e-494d141f086e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] <<SYS>>\n",
            "You are an honest assistant , who gives factually correct answers while refering to the context. If the context does not have an answer you try your best to answer the question on your own.\n",
            "\n",
            "Answer the following question while refering the context.If the answer is not in context ,try to answer on your own . Still if you are not able to answer the question then politely say so instead of ramblling.\n",
            "\n",
            "<</SYS>>\n",
            "Context: figure 1 : the transformer - model architecture. the transformer follows this overall architecture using stacked self - attention and point - wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of figure 1, respectively. 3. 1 encoder and decoder stacks encoder : the encoder is composed of a stack of n = 6 identical layers. each layer has two sub - layers. the first is a multi - head self -. in this work we employ h = 8 parallel attention layers, or heads. for each of these we use dk = dv = dmodel / h = 64. due to the reduced dimension of each head, the total computational cost is similar to that of single - head attention with full dimensionality. 3. 2. 3 applications of attention in our model the transformer uses multi - head attention in three different ways : • in \" encoder - decoder attention \" layers,\n",
            "Qustion:What are the parts of transformers?\n",
            " [/INST]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown"
      ],
      "metadata": {
        "id": "4v9cy75nnBCx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = 'I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?'\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\",padding =True).to(device)\n",
        "generate_ids = model.generate(inputs.input_ids, max_length=5000,top_k=1,top_p=0.5,temperature=0.01)\n"
      ],
      "metadata": {
        "id": "mnDdoWpkmt3h"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skipping the prompt from the result."
      ],
      "metadata": {
        "id": "qbHjr5KwOT2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p=generate_ids[0][inputs['input_ids'].shape[1]:]"
      ],
      "metadata": {
        "id": "dz3YW0TPpzm6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Markdown for better outputs.\n",
        "Results are quite astonishing for a 7b parameter model."
      ],
      "metadata": {
        "id": "5wk3zYYBOeQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(tokenizer.decode(p, skip_special_tokens=True, clean_up_tokenization_spaces=True))\n",
        "# print(tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])"
      ],
      "metadata": {
        "id": "lgFNiR36pkpR",
        "outputId": "db9106a2-656c-4b1b-8630-4a614ee8599b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Based on the context provided, the parts of the Transformer model can be identified as follows:\n1. Encoder: The encoder is composed of a stack of $n$ identical layers, each layer having two sub-layers. The first sub-layer is a multi-head self-attention layer, and the second sub-layer is a point-wise, fully connected layer.\n2. Decoder: The decoder is also composed of a stack of $n$ identical layers, each layer having two sub-layers. The first sub-layer is a multi-head self-attention layer, and the second sub-layer is a point-wise, fully connected layer.\n3. Encoder and decoder stacks: The encoder and decoder stacks are the layers that are stacked together to form the Transformer model. The encoder stack is responsible for encoding the input sequence, and the decoder stack is responsible for decoding the output sequence.\n4. Multi-head attention: The Transformer model uses multi-head attention in both the encoder and decoder. This allows the model to attend to different parts of the input sequence simultaneously and capture complex contextual relationships.\n5. Self-attention: The Transformer model uses self-attention mechanisms to allow the model to attend to different parts of the input sequence. This is done by computing the attention weights between each element in the input sequence and all other elements in the sequence.\n6. Point-wise, fully connected layers: The Transformer model uses point-wise, fully connected layers to process the output of the self-attention mechanism. These layers are responsible for transforming the output of the self-attention mechanism into the final output of the Transformer model.\n7. D Dimension: The Transformer model uses a dimension of $d$ to represent the input sequence. This dimension is used to compute the attention weights and to process the output of the self-attention mechanism.\n8. H Dimension: The Transformer model uses a dimension of $h$ to represent the number of attention heads. This allows the model to attend to different parts of the input sequence simultaneously and capture complex contextual relationships.\n9. DK Dimension: The Transformer model uses a dimension of $dk$ to represent the dimensionality of the key and value vectors. This allows the model to capture complex contextual relationships in the input sequence.\n10. DV Dimension: The Transformer model uses a dimension of $dv$ to represent the dimensionality of the output vector. This allows the model to produce a final output vector that captures the contextual relationships in the input sequence.\n\nIn summary, the parts of the Transformer model are:\n\n* Encoder and decoder stacks\n* Multi-head attention\n* Self-attention\n* Point-wise, fully connected layers\n* D Dimension\n* H Dimension\n* DK Dimension\n* DV Dimension\n\nI hope this helps! Let me know if you have any further questions."
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part 2 : Creating Chain using Langchain"
      ],
      "metadata": {
        "id": "CLg8v14UbsPw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n"
      ],
      "metadata": {
        "id": "sXFI3N-GjUQJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFacePipeline\n"
      ],
      "metadata": {
        "id": "FW9ljglBKf9h"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=\"\"\"<s>[INST] <<SYS>>\n",
        "You are an honest assistant , who gives factually correct answers while refering to the context. If the context does not have an answer you try your best to answer the question on your own.\\n\n",
        "Answer the following question while refering the context.If the answer is not in context ,try to answer on your own . Still if you are not able to answer the question then politely say so instead of ramblling.\\n\n",
        "<</SYS>>\n",
        "Context:{context}\n",
        "Qustion:{question}\n",
        " [/INST]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "lRxXpMx4kSio"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_t=PromptTemplate(input_variables=['context','question'],template=prompt)\n",
        "prompt_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RObQNMOhkCbg",
        "outputId": "70d3947a-1c55-489d-9e0c-9c2a0a904ce4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='<s>[INST] <<SYS>>\\nYou are an honest assistant , who gives factually correct answers while refering to the context. If the context does not have an answer you try your best to answer the question on your own.\\n\\nAnswer the following question while refering the context.If the answer is not in context ,try to answer on your own . Still if you are not able to answer the question then politely say so instead of ramblling.\\n\\n<</SYS>>\\nContext:{context}\\nQustion:{question}\\n [/INST]\\n', template_format='f-string', validate_template=True)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qp=prompt_t.format(context=context,question=question)"
      ],
      "metadata": {
        "id": "-yPhrPN2Xv8n"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe=pipeline(task='text-generation',model=model,tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "cqkF1OJfE9dL",
        "outputId": "94791907-cc79-4083-fd0f-c07205c27a87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = pipe(prompt, num_return_sequences=1, return_full_text=False)\n",
        "outputs\n",
        "Markdown(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "QbieuzHWHe5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_llm=HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "id": "JfJud1rXGZDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain=LLMChain(llm=local_llm,prompt=prompt_t)"
      ],
      "metadata": {
        "id": "4TEaEU0NjlDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.predict(context=context,question=question)"
      ],
      "metadata": {
        "id": "3yZtPrxpLkL3",
        "outputId": "57129fe8-1596-45b8-851d-3fb2d88c0b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (4096) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Based on the context you provided, the parts of the Transformer model are:\\n\\n1. Encoder: The encoder is composed of a stack of $n$ identical layers, where each layer has two sub-layers. The first sub-layer is a multi-head self-attention layer, and the second sub-layer is a point-wise, fully connected layer.\\n2. Decoder: The decoder is also composed of a stack of $n$ identical layers, where each layer has two sub-layers. The first sub-layer is a multi-head self-attention layer, and the second sub-layer is a point-wise, fully connected layer.\\n3. Encoder-decoder attention: The Transformer uses multi-head attention in this way, where the encoder and decoder attend to each other's output.\\n\\nPlease let me know if you have any further questions!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Cust_chain.py\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.document_loaders import UnstructuredPDFLoader\n",
        "from langchain.document_loaders import PDFMinerPDFasHTMLLoader\n",
        "from langchain.document_loaders import PDFPlumberLoader\n",
        "from langchain.text_splitter import TokenTextSplitter\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from langchain.document_loaders import OnlinePDFLoader\n",
        "\n",
        "\n",
        "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from transformers import pipeline\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "class Cust_Chain_obj():\n",
        "  def __init__(self,model,tokenizer,FAISS_obj,sys_prompt='',embeddings = HuggingFaceEmbeddings(),text_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=50)):\n",
        "    self.FAISS_obj=FAISS_obj\n",
        "    self.model=model\n",
        "    self.tokenizer=tokenizer\n",
        "    self.sys_prompt=sys_prompt\n",
        "    self.embedding_obj=embeddings\n",
        "    self.text_splitter_obj=text_splitter\n",
        "    self.doc=[]\n",
        "    self.prompt_template=self.create_prompt_template()\n",
        "  def create_prompt_template(self):\n",
        "    prompt=\"\"\"<s>[INST] \"\"\"+self.sys_prompt+\"\"\"\n",
        "        Context:{context}\n",
        "        Qustion:{question}\n",
        "        [/INST]\n",
        "        \"\"\"\n",
        "    prompt_t=PromptTemplate(input_variables=['context','question'],template=prompt)\n",
        "    return prompt_t\n",
        "\n",
        "  def load_pdf_doc(self,doc_path):\n",
        "    loader=PyPDFLoader(urls)\n",
        "    data=loader.load()\n",
        "    self.doc=data\n",
        "    return self.doc\n",
        "\n",
        "  def set_embedding_obj(self,new_embedding_obj):\n",
        "    self.embedding_obj=new_embedding_obj\n",
        "\n",
        "  def set_sys_prompt(self,new_sys_prompt):\n",
        "    self.sys_prompt=f\"<<SYS>>{new_sys_prompt}<</SYS>>\"\n",
        "    self.prompt_template=self.create_prompt_template()\n",
        "\n",
        "  def set_new_model(self,new_model):\n",
        "    self.model=new_model\n",
        "\n",
        "  def populate_vector_store(self,document=None,embedding_obj=None,text_splitter=None):\n",
        "    if(document==None):\n",
        "      document=self.doc\n",
        "    if embedding_obj is None:\n",
        "      embedding_obj=self.embedding_obj\n",
        "    if (text_splitter is None):\n",
        "      text_splitter=self.text_splitter_obj\n",
        "    docs=text_splitter.split_documents(document)\n",
        "    self.FAISS_obj=FAISS.from_documents(docs,embedding_obj)\n",
        "\n",
        "  def retrieve_documents(self,query,k=2):\n",
        "    contexts=self.FAISS_obj.similarity_search(query,k=k)\n",
        "    return contexts\n",
        "\n",
        "  def retrieve_contexts(self,query,k=2,use_contexts=False,cust_context='')  :\n",
        "    context = \"No context available ,answer on your own.\\n\"\n",
        "    meta_datas=[]\n",
        "    if(use_contexts==True):\n",
        "      contexts=self.retrieve_documents(query,k)\n",
        "      cc=''\n",
        "      for c in contexts:\n",
        "        cc+=c.page_content\n",
        "        meta_datas.append(c.metadata)\n",
        "        cc+='\\n'\n",
        "      if(len(cc)!=0  ):\n",
        "        context=cc\n",
        "      context+=cust_context\n",
        "    return context,meta_datas\n",
        "\n",
        "  def run(self,query,k=2,use_contexts_from_doc=False,cust_context=''):\n",
        "    context,metadata=self.retrieve_contexts(query,k,use_contexts_from_doc,cust_context)\n",
        "    prompt=self.prompt_template.format(context=context,question=query)\n",
        "    inputs = self.tokenizer(prompt, return_tensors=\"pt\",padding =True).to(device)\n",
        "    generate_ids = self.model.generate(inputs.input_ids, max_length=4000,top_k=1,top_p=0.5,temperature=0.01)\n",
        "    p=generate_ids[0][inputs['input_ids'].shape[1]:]\n",
        "    output=self.tokenizer.decode(p, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    return metadata,output\n"
      ],
      "metadata": {
        "id": "yVnBOjSWorIi",
        "collapsed": true,
        "outputId": "c025204d-ac0f-4a54-bc5b-348662ad2709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Cust_chain.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "urls=\"https://arxiv.org/pdf/1706.03762.pdf\"\n",
        "loader=PyPDFLoader(urls)\n",
        "data=loader.load()\n",
        "sys_prompt=\"\"\"<<SYS>>You are an honest assistant , who gives factually correct answers while refering to the context. If the context does not have an answer or if context is not available you try your best to answer the question on your own.<</SYS>>\"\"\""
      ],
      "metadata": {
        "id": "Xklu17t0Zm2v"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain=Cust_Chain_obj(model,tokenizer,None,sys_prompt)\n",
        "chain.load_pdf_doc(urls)\n",
        "chain.populate_vector_store()"
      ],
      "metadata": {
        "id": "J0YUcKGtygWu"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.prompt_template"
      ],
      "metadata": {
        "id": "timCx41_ir68",
        "outputId": "db16277a-06d5-4cde-b467-15d186e63e44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template='<s>[INST] <<SYS>>You are an honest assistant , who gives factually correct answers while refering to the context. If the context does not have an answer or if context is not available you try your best to answer the question on your own.<</SYS>>\\n        Context:{context}\\n        Qustion:{question}\\n        [/INST]\\n        ', template_format='f-string', validate_template=True)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain.retrieve_contexts('What are transformers?',2,True)"
      ],
      "metadata": {
        "id": "iFi6xNW6cFdl",
        "outputId": "f7a515d2-4cec-4c36-ad30-1d2abe8b7627",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('figure 1 : the transformer - model architecture. the transformer follows this overall architecture using stacked self - attention and point - wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of figure 1, respectively. 3. 1 encoder and decoder stacks encoder : the encoder is composed of a stack of n = 6 identical layers. each layer has two sub - layers. the first is a multi - head self - attention mechanism, and the second is a simple, position - wise fully connected feed - forward network. we employ a residual connection [ 11 ] around each of the two sub - layers, followed by layer normalization [ 1 ]. that is, the output of each sub - layer is layernorm ( x + sublayer ( x ) ), where sublayer ( x ) is the function implemented by the sub - layer itself. to facilitate these residual connections, all sub - layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. decoder : the decoder is also composed of a stack of n = 6identical layers. in addition to the two sub - layers in each encoder layer, the decoder inserts a third sub - layer, which performs multi - head attention over the output of the encoder stack. similar to the encoder, we employ residual connections around each of the sub - layers, followed by layer normalization. we also modify the self - attention sub - layer in the decoder stack to prevent positions from attending to subsequent positions. this masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3. 2 attention an attention function can be described as mapping\\n- the - art bleu score of 41. 8 after training for 3. 5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data. ∗equal contribution. listing order is random. jakob proposed replacing rnns with self - attention and started the effort to evaluate this idea. ashish, with illia, designed and implemented the first transformer models and has been crucially involved in every aspect of this work. noam proposed scaled dot - product attention, multi - head attention and the parameter - free position representation and became the other person involved in nearly every detail. niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. lukasz and aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. † work performed while at google brain. ‡ work performed while at google research. 31st conference on neural information processing systems ( nips 2017 ), long beach, ca, usa. arxiv : 1706. 03762v7 [ cs. cl ] 2 aug 2023\\n',\n",
              " [{'source': '/tmp/tmphsoc0wr9/tmp.pdf', 'page': 2},\n",
              "  {'source': '/tmp/tmphsoc0wr9/tmp.pdf', 'page': 0}])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(chain.run('What are Transformers?',use_contexts_from_doc=True)[1])"
      ],
      "metadata": {
        "id": "Wqhb8mDxcsW9",
        "outputId": "94289d3e-eadd-4031-efab-667e705d3269",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] <<SYS>>You are an honest assistant , who gives factually correct answers while refering to the context. If the context does not have an answer or if context is not available you try your best to answer the question on your own.<</SYS>>\n",
            "        Context:figure 1 : the transformer - model architecture. the transformer follows this overall architecture using stacked self - attention and point - wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of figure 1, respectively. 3. 1 encoder and decoder stacks encoder : the encoder is composed of a stack of n = 6 identical layers. each layer has two sub - layers. the first is a multi - head self - attention mechanism, and the second is a simple, position - wise fully connected feed - forward network. we employ a residual connection [ 11 ] around each of the two sub - layers, followed by layer normalization [ 1 ]. that is, the output of each sub - layer is layernorm ( x + sublayer ( x ) ), where sublayer ( x ) is the function implemented by the sub - layer itself. to facilitate these residual connections, all sub - layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. decoder : the decoder is also composed of a stack of n = 6identical layers. in addition to the two sub - layers in each encoder layer, the decoder inserts a third sub - layer, which performs multi - head attention over the output of the encoder stack. similar to the encoder, we employ residual connections around each of the sub - layers, followed by layer normalization. we also modify the self - attention sub - layer in the decoder stack to prevent positions from attending to subsequent positions. this masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position ican depend only on the known outputs at positions less than i. 3. 2 attention an attention function can be described as mapping\n",
            "- the - art bleu score of 41. 8 after training for 3. 5 days on eight gpus, a small fraction of the training costs of the best models from the literature. we show that the transformer generalizes well to other tasks by applying it successfully to english constituency parsing both with large and limited training data. ∗equal contribution. listing order is random. jakob proposed replacing rnns with self - attention and started the effort to evaluate this idea. ashish, with illia, designed and implemented the first transformer models and has been crucially involved in every aspect of this work. noam proposed scaled dot - product attention, multi - head attention and the parameter - free position representation and became the other person involved in nearly every detail. niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. lukasz and aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. † work performed while at google brain. ‡ work performed while at google research. 31st conference on neural information processing systems ( nips 2017 ), long beach, ca, usa. arxiv : 1706. 03762v7 [ cs. cl ] 2 aug 2023\n",
            "\n",
            "        Qustion:What are Transformers?\n",
            "        [/INST]\n",
            "        \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Transformers are a type of neural network architecture introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. The Transformer model relies solely on self-attention mechanisms, eliminating the need for traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs).\nThe Transformer architecture consists of an encoder and a decoder, each composed of multiple identical layers. Each layer in the encoder and decoder consists of two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The self-attention mechanism allows the model to attend to different parts of the input sequence simultaneously and weigh their importance, while the feed-forward network processes the output of the self-attention mechanism to produce the final output.\nOne of the key innovations of the Transformer is the use of residual connections, which allow the model to learn more complex and abstract representations of the input sequence. This is achieved by adding the input of each layer to its output, effectively creating a residual connection between the two.\nAnother important aspect of the Transformer is the use of attention mechanisms, which allow the model to selectively focus on different parts of the input sequence as it processes it. This is achieved through the use of a query, key, and value matrix, which are used to compute the attention weights and produce the final output.\nThe Transformer has been highly successful in a variety of natural language processing tasks, including machine translation, text generation, and question answering. Its ability to process input sequences of arbitrary length and its parallelization capabilities make it a powerful tool for large-scale data processing.\nIn summary, Transformers are a type of neural network architecture that relies solely on self-attention mechanisms to process input sequences. They are composed of encoder and decoder layers, each consisting of multiple sub-layers, and use residual connections and attention mechanisms to learn complex and abstract representations of the input sequence."
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CKq7mEXqesqD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}